import json
import re
import pickle
import numpy as np
import os

# import transformer.Constants as Constants
# !/usr/bin/env python
# -*-encoding:UTF-8 -*-
import database.Constants as Constants

min_time = 1098028800.0

f = open(os.getcwd() + '/Yelp/Yelp_check_ins.txt', 'r')

city = Constants.NOW_CITY
# ingoing = np.load("Yelp/%s_ingoing.npy" % city)
# outgoing = np.load("Yelp/%s_outgoing.npy" % city)
disc = np.load("Yelp/%s_disc.npy" % city)
user_group = np.load("Yelp/%s_user_group.npy" % city)

train_data = []
test_data = []
# max_type = 999
user_index = 0
actions = []
time_s = 0


group = np.load("Yelp/%s_100_kmeans.npy" % city)
train_track = np.load("Yelp/%s_train_track.npy" % city)
test_track = np.load("Yelp/%s_test_track.npy" % city)

a1 = 0
a2 = 0

poi = 0
poi2 = 0

line = f.readline()
while line:
    line = line.split("\n")[0]
    data = line.split("\t")
    # print(user_index,int(data[0] ))
    if user_index != int(data[0]):
        # train_data.append(actions)
        user_index += 1
        time_s = 0
        len_ = len(actions)
        tuning_len = int(len_*0.7)
        test_len = int(len_*0.9)

        if len_ >= 10:

            tuning_label = []
            last_ = actions[tuning_len-1]['type_event']
            t = np.zeros(Constants.POI_NUM)
            for l in actions[:tuning_len]:
                t += train_track[l['type_event']]

            dis = disc[last_]
            where_ = np.where(dis<20)[0]
            where_2 = np.intersect1d(where_, np.where(t>0.05)[0])

            # print(where_.shape, where_2.shape)
            t = t[where_2]
            dis = disc[last_][where_2]

            for l in actions[tuning_len:test_len]:
                if not tuning_label.__contains__(l['type_event']):
                    tuning_label.append(l['type_event'])

            include_count = 0
            include_count2 = 0
            poi += where_.shape[0]
            poi2 += where_2.shape[0]
            for label in tuning_label:
                if where_.__contains__(label):
                    # print('include:', label)
                    include_count += 1
                else:
                    # print('really really bad, not include ! ', train_track[actions[0]['type_event']][label] )
                    if where_.__contains__(label):
                        print('however where included it')

                if where_2.__contains__(label):
                    # print('include:', label)
                    include_count2 += 1

            a1 += include_count / len(tuning_label)
            a2 += include_count2 / len(tuning_label)
            # print('percent: ', include_count / len(tuning_label), include_count2 / len(tuning_label))

            # print(t.shape, dis.shape)
            # print(tuning_label)
            train_data.append({'actions': actions[:tuning_len],
                             'label': tuning_label, 'group_': where_2, 'distance': dis, 'track':t})

            # test_data
            test_label = []
            # last_ = actions[test_len-1]['type_event']
            # where_ = np.where(disc[last_]<20)[0]

            last_ = actions[test_len-1]['type_event']
            t = np.zeros(Constants.POI_NUM)
            for l in actions[:test_len]:
                t += test_track[l['type_event']]

            dis = disc[last_]
            where_ = np.where(dis<20)[0]
            where_2 = np.intersect1d(where_, np.where(t>0.05)[0])

            t = t[where_2]
            dis = disc[last_][where_2]

            for l in actions[test_len:]:
                if not test_label.__contains__(l['type_event']):
                    test_label.append(l['type_event'])

            # where_ = np.where(t != 0)[0]
            test_data.append({'actions': actions[:test_len],
                             'label': test_label, 'group_': where_2, 'distance': dis, 'track':t})

        actions = []

        if user_index % 1000 == 0:
            print(user_index)

    # print(int(data[1]))
    # print((float(data[2])-min_time)/3600000+time_s)

    actions.append({'time_since_start': (float(data[2])-min_time)/3600000+time_s,
                             'time_since_last_event': time_s if len(actions) != 0 else 0,
                             'type_event': int(data[1]),
                             'event_user_group': group[int(data[0])]
                             })
    #                        'type_event': max_type if int(data[1]) > max_type else int(data[1])})

    time_s += 1

    line = f.readline()
        # break
    # if user_index == 1:
    #     print(train_data)
    #     break

print('a1', a1/user_index)
print('a2', a2/user_index)

print('poi', poi/user_index)
print('poi2', poi2/user_index)
f.close()
#
print('train_data', len(train_data))
print('test_data', len(test_data))
# print(train_data)

# data = {'dim_process': Constants.TYPE_DIMENSION, 'train': train_data[:int(len(train_data)*0.8)]}

# disc = np.load("Yelp/%s_disc.npy" % city)
ingoing = np.load("Yelp/%s_ingoing.npy" % city)
outgoing = np.load("Yelp/%s_outgoing.npy" % city)


# np.random.shuffle(train_data)

with open("Yelp/%s_all_group.pkl" % city, 'rb') as f:
    # data = pickle.load(f, encoding='latin-1')

    data = {'dim_process': Constants.POI_NUM, 'train': train_data,
            'num_groups': Constants.NUM_GROUP, 'key_group': [], 'value_group': [],
            'group':group, 'ingoing': ingoing, 'outgoing': outgoing}

    # print(data['train'].__len__())

    data_output = open(os.getcwd() + '/Yelp/train_%s.pkl'%city,'wb')
    pickle.dump(data, data_output)
    data_output.close()

    # data_output = open(os.getcwd() + '/data/Yelp/dev.pkl','wb')
    # pickle.dump(train_data, data_output)
    # data_output.close()

    data = {'dim_process': Constants.POI_NUM, 'test': test_data,
            'num_groups': Constants.NUM_GROUP, 'key_group': [], 'value_group': [],
            'group':group, 'ingoing': ingoing, 'outgoing': outgoing}

    # print(data['train'].__len__())

    data_output = open(os.getcwd() + '/Yelp/test_%s.pkl'%city,'wb')
    pickle.dump(data, data_output)
    data_output.close()
#
# error = []
# f = open(os.getcwd() + '/data/Yelp/Yelp_check_ins_categories.txt', 'w')
# for er in error:
#     f.write(er+"\r\n")
#
# print(len(error))

